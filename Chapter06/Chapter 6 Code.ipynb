{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9b4e65-714c-46a7-b0ca-2c22d87e349e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chapter 6: SQL Queries in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b6f89b-2d68-4937-a30a-ac64ef7caa49",
     "showTitle": true,
     "title": "Create Salary dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  4| Michael|     Sales|  3000| 20|\n|  5|   Kelly|   Finance|  3500| 35|\n|  6|    Kate|   Finance|  3000| 45|\n|  7|  Martin|   Finance|  3500| 26|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500, 40), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000, 38), \\\n",
    "    (3, \"Maria\", \"Finance\", 3500, 28), \\\n",
    "    (4, \"Michael\", \"Sales\", 3000, 20), \\\n",
    "    (5, \"Kelly\", \"Finance\", 3500, 35), \\\n",
    "    (6, \"Kate\", \"Finance\", 3000, 45), \\\n",
    "    (7, \"Martin\", \"Finance\", 3500, 26), \\\n",
    "    (8, \"Kiran\", \"Sales\", 2200, 35), \\\n",
    "  ]\n",
    "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\", \"Age\"]\n",
    "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
    "salary_data_with_id.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e075be3c-bb49-4c81-a7b9-7dfbb4370056",
     "showTitle": true,
     "title": "Writing csv file"
    }
   },
   "outputs": [],
   "source": [
    "salary_data_with_id.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"salary_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11fe3af-e723-4e97-abbf-d503acf033e3",
     "showTitle": true,
     "title": "Reading csv file"
    }
   },
   "outputs": [],
   "source": [
    "csv_data = spark.read.csv('/salary_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1aa8d82-a393-448a-8fd0-5110b1eb1af2",
     "showTitle": true,
     "title": "Showing data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  4| Michael|     Sales|  3000| 20|\n|  5|   Kelly|   Finance|  3500| 35|\n|  6|    Kate|   Finance|  3000| 45|\n|  7|  Martin|   Finance|  3500| 26|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "csv_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f186780-97f0-4b7f-af17-c33073c872ce",
     "showTitle": true,
     "title": "# Perform transformations on the loaded data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  5|   Kelly|   Finance|  3500| 35|\n|  7|  Martin|   Finance|  3500| 26|\n+---+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform transformations on the loaded data \n",
    "processed_data = csv_data.filter(csv_data[\"Salary\"] > 3000) \n",
    "# Save the processed data as a table \n",
    "processed_data.createOrReplaceTempView(\"high_salary_employees\") \n",
    "# Perform SQL queries on the saved table \n",
    "results = spark.sql(\"SELECT * FROM high_salary_employees \") \n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022c96b3-91f2-41ea-91c4-58ecd511a2f6",
     "showTitle": true,
     "title": "Saving Transformed Data as a View"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|    John| Field-eng|  3500| 40|\n|  Robert|     Sales|  4000| 38|\n|   Kelly|   Finance|  3500| 35|\n|    Kate|   Finance|  3000| 45|\n|   Kiran|     Sales|  2200| 35|\n+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data as a view \n",
    "salary_data_with_id.createOrReplaceTempView(\"employees\") \n",
    "#Apply filtering on data\n",
    "filtered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30\") \n",
    "# Display the results \n",
    "filtered_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174c84ac-fa9e-42a7-a3b5-a166193e63b0",
     "showTitle": true,
     "title": "Aggregating data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|average_salary|\n+--------------+\n|        3275.0|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform an aggregation to calculate the average salary \n",
    "average_salary = spark.sql(\"SELECT AVG(Salary) AS average_salary FROM employees\") \n",
    "# Display the average salary \n",
    "average_salary.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bfd3f0-1a65-4a58-bdc7-58b55dd58840",
     "showTitle": true,
     "title": "Sorting data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  2|  Robert|     Sales|  4000| 38|\n|  1|    John| Field-eng|  3500| 40|\n|  7|  Martin|   Finance|  3500| 26|\n|  3|   Maria|   Finance|  3500| 28|\n|  5|   Kelly|   Finance|  3500| 35|\n|  4| Michael|     Sales|  3000| 20|\n|  6|    Kate|   Finance|  3000| 45|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort the data based on the salary column in descending order \n",
    "sorted_data = spark.sql(\"SELECT * FROM employees ORDER BY Salary DESC\") \n",
    "# Display the sorted data \n",
    "sorted_data.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38606a23-aa13-49ca-b7dc-d669dd472f55",
     "showTitle": true,
     "title": "Combining Aggregations"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|  Robert|     Sales|  4000| 38|\n|    John| Field-eng|  3500| 40|\n|   Kelly|   Finance|  3500| 35|\n+--------+----------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort the data based on the salary column in descending order \n",
    "filtered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30 AND Salary > 3000 ORDER BY Salary DESC\") \n",
    "# Display the results \n",
    "filtered_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7701c892-0883-4bc2-9b5e-f51cf55fcc78",
     "showTitle": true,
     "title": "Grouping data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|Department|       avg(Salary)|\n+----------+------------------+\n|     Sales|3066.6666666666665|\n|   Finance|            3375.0|\n| Field-eng|            3500.0|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on the Department column and take average salary for each department  \n",
    "grouped_data = spark.sql(\"SELECT Department, avg(Salary) FROM employees GROUP BY Department\") \n",
    "# Display the results \n",
    "grouped_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abafb986-88fa-4c43-8f65-6f7bbfa70ec6",
     "showTitle": true,
     "title": "Grouping with multiple aggregations"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n|Department|total_salary|max_salary|\n+----------+------------+----------+\n|     Sales|        9200|      4000|\n|   Finance|       13500|      3500|\n| Field-eng|        3500|      3500|\n+----------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform grouping and multiple aggregations  \n",
    "aggregated_data = spark.sql(\"SELECT Department, sum(Salary) AS total_salary, max(Salary) AS max_salary FROM employees GROUP BY Department\") \n",
    "\n",
    "# Display the results \n",
    "aggregated_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b85baa3-2e70-4038-af6c-440346d96d78",
     "showTitle": true,
     "title": "Window functions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+--------------+\n| ID|Employee|Department|Salary|Age|cumulative_sum|\n+---+--------+----------+------+---+--------------+\n|  1|    John| Field-eng|  3500| 40|          3500|\n|  7|  Martin|   Finance|  3500| 26|          3500|\n|  3|   Maria|   Finance|  3500| 28|          7000|\n|  5|   Kelly|   Finance|  3500| 35|         10500|\n|  6|    Kate|   Finance|  3000| 45|         13500|\n|  4| Michael|     Sales|  3000| 20|          3000|\n|  8|   Kiran|     Sales|  2200| 35|          5200|\n|  2|  Robert|     Sales|  4000| 38|          9200|\n+---+--------+----------+------+---+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n",
    "\n",
    "# Calculate the cumulative sum using window function\n",
    "df_with_cumulative_sum = salary_data_with_id.withColumn(\"cumulative_sum\", sum(col(\"Salary\")).over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "df_with_cumulative_sum.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c6978-6e33-47db-8ad6-8af7cd77d522",
     "showTitle": true,
     "title": "Using udfs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+----------------+\n| ID|Employee|Department|Salary|Age|capitalized_name|\n+---+--------+----------+------+---+----------------+\n|  1|    John| Field-eng|  3500| 40|            JOHN|\n|  2|  Robert|     Sales|  4000| 38|          ROBERT|\n|  3|   Maria|   Finance|  3500| 28|           MARIA|\n|  4| Michael|     Sales|  3000| 20|         MICHAEL|\n|  5|   Kelly|   Finance|  3500| 35|           KELLY|\n|  6|    Kate|   Finance|  3000| 45|            KATE|\n|  7|  Martin|   Finance|  3500| 26|          MARTIN|\n|  8|   Kiran|     Sales|  2200| 35|           KIRAN|\n+---+--------+----------+------+---+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to capitalize a string\n",
    "capitalize_udf = udf(lambda x: x.upper(), StringType())\n",
    "\n",
    "# Apply the UDF to a column\n",
    "df_with_capitalized_names = salary_data_with_id.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
    "\n",
    "# Display the result\n",
    "df_with_capitalized_names.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19166b9b-a9c2-4e89-959f-abbac8ef8eff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+----------------+\n| ID|Employee|Department|Salary|Age|capitalized_name|\n+---+--------+----------+------+---+----------------+\n|  1|    John| Field-eng|  3500| 40|            JOHN|\n|  2|  Robert|     Sales|  4000| 38|          ROBERT|\n|  3|   Maria|   Finance|  3500| 28|           MARIA|\n|  4| Michael|     Sales|  3000| 20|         MICHAEL|\n|  5|   Kelly|   Finance|  3500| 35|           KELLY|\n|  6|    Kate|   Finance|  3000| 45|            KATE|\n|  7|  Martin|   Finance|  3500| 26|          MARTIN|\n|  8|   Kiran|     Sales|  2200| 35|           KIRAN|\n+---+--------+----------+------+---+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to capitalize a string\n",
    "capitalize_udf = udf(lambda x: x.upper(), StringType())\n",
    "\n",
    "# Apply the UDF to a column\n",
    "df_with_capitalized_names = salary_data_with_id.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
    "\n",
    "# Display the result\n",
    "df_with_capitalized_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1d9134-f8bf-4597-9b46-1e0142a0acac",
     "showTitle": true,
     "title": "Applying functions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|pandas_plus_one(Salary)|\n+-----------------------+\n|                   3501|\n|                   4001|\n|                   3501|\n|                   3001|\n|                   3501|\n|                   3001|\n|                   3501|\n|                   2201|\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "salary_data_with_id.select(pandas_plus_one(salary_data_with_id.Salary)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0141f5b-f209-4f66-928f-1d631a99ca58",
     "showTitle": true,
     "title": "Pandas udfs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|add_one(Salary)|\n+---------------+\n|           3501|\n|           4001|\n|           3501|\n|           3001|\n|           3501|\n|           3001|\n|           3501|\n|           2201|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"SELECT add_one(Salary) FROM employees\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6c3ea0-f650-4806-93cf-0368b01c2dd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Chapter 6 Code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
